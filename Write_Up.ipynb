{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of Continual Learning \n",
    "\n",
    "_Andres Castellano, Benedict Waiharo, Alan Yueng_\n",
    "        \n",
    "\n",
    "## Background\n",
    "\n",
    "The basic premise of a machine learning, or deep learning algorithm, is to familiarize a computer with the numerical representation of processes or objects. For example: in a regression problem, a programmer will show a computer a set of target variables and a set of features that need to be used to predict that target variable. In addition, the programmer will give the computer a routine by which the computer needs to learn. The computer will perform the routine and update its assumptions itireatively until the computer and programmer are both satisfied with the accuracy of the model created to predict the target variable. A different and perhaps more concrete example is that of a classification algorithm. Imagine that a program needs to be created to classify images of cats versus dogs. In this case, the programmer feeds the computer a set of images where some images are cats and others are dogs, and each image is labeled accordingly. Then the programmer gives the computer a way to convert those images into numerical abstractions, and a routine by which to learn what a cat or dog looks like, based on those numerical abstractions. Current Artificial Neural Networks, in particular Convolutional Networks are highly effective at learning these representations. So imagine gathering hundreds of thousands of images of Chihuahuas and all cat breeds. There is a strong possibility that the algorithm you have trained is quite accurate at determining cats or dogs from images. Or is it?\n",
    "    \n",
    "### The Problem\n",
    "\n",
    "Recall that in the cat or dog example, images of all cat breeds and Chihuahuas were used to train the Network. So what happends when the computer is given an image of a German Shepperd? Could it possibly determine that it is a dog? Possibly, but that is not good enough. The computer needs to be able to determine dog with the minimum possible margin of error. What to do? One alternative is to retrain the network with images of Chihuahuas, German Shepperds, and all cat breeds. But then what about Rottweilers? The network can be retrained with images of Chihuahuas, German Shepperds, Rottweilers and all cat breeds. But what about Labradors? At this point, you should be yelling \"Just train the network with all dog breeds!\" Yes but what about pet grooming? Do all French Poodles look the same? The problem here is that even the most basic problems present challenges as to what a representative dataset looks like. It is unlikely that enough images can be collected of every single breed and every single grooming style. Moreover, training neural networks can be challenging and time consuming. It is impractical to think of re-training a network every single time a new data point is introduced.\n",
    "    \n",
    "The example above is simple and perhaps mundane, but imagine other more critical applications in fields that are rapidly changing. For example, the healthcare industry in which new drugs or ailments are discovered constantly. \n",
    "\n",
    "More importantly, consider how human beings learn. No human alive has ever seen every version of a dog, yet if shown a dog breed that she hasn't seen before, she can quickly tell one, that it is in fact a dog, and two she can learn that another dog breed exists and what it looks like. This ability of animals to learn from new data is critical in the real world. As humans increasingly rely on machines to perform more tasks, it follows that sooner rather than later, machines will need to be able to continuosly learn in real time.\n",
    "    \n",
    "### The Alternative and its Problem\n",
    "\n",
    "In the cat or dog example, suppose that instead of re-training the network, you only train it a little bit more. That is instead of starting from scratch, the network is further trained to learn what a German Shepperd looks like in addition to what it already knows about Chihuahuas and Cats. In other words, it _continues_ to learn. Great! Except, research has shown that the performance of conventional neural networks on previously learned tasks significantly decrease as new tasks are learned (Kemker et al. 2018, Maltoni & Lomonaco 2018). This problem is commonly refered to as catastrophic learning or interference. That is, as the model is trained to perform new tasks, it forgets how to execute old tasks or worse, it confuses its tasks to the point where it might tag a Chihuahua as a cat.\n",
    "\n",
    "__Kemker, R., McClure, M., Abitino, A., Hayes, T. & Kanan, C. (2018), Measuring catastrophic\n",
    "forgetting in neural networks, AAAIâ€™18, New Orleans, LA.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "\n",
    "In the most basic terms, the challenge is to develop a system or process by which an algorithm can be trained on a dataset, and simultaneously give it the ability to learn new tasks without forgetting how to perform previously learned tasks or confuse new tasks with previously learned tasks.\n",
    "\n",
    "More concretely, \"for overcoming catastrophic forgetting, learning systems must, on the one hand, show the ability to\n",
    "acquire new knowledge and refine existing knowledge on the basis of the continuous input and, on\n",
    "the other hand, prevent the novel input from significantly interfering with existing knowledge. The\n",
    "extent to which a system must be plastic in order to integrate novel information and stable in order\n",
    "not to catastrophically interfere with consolidated knowledge is known as the stability-plasticity\n",
    "dilemma and has been widely studied in both biological systems and computational models\" (Parisi et al. 2019).\n",
    "\n",
    "\n",
    "__Parisi, G., Kemker, R., Part, J., Kanan, C., & Wermter, S. (2019), 'Continual Lifelong Learning with Neural Networks: A    Review', \tarXiv:1802.07569 [cs.LG]__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifelong Learning and Catastrophic Forgetting in Neural Networks.\n",
    "\n",
    "### Lifelong Machine Learning\n",
    "\n",
    "As discussed before, machine learning models tend to suffer from catastrophic learning as they learn from novel observations. Based on traditional methods, it is likely that after learning several novel tasks, the model will be absolutely incapable of permorming the tasks it was originally trained to perform. However, as data becomes increasingly available, ML models need to be able to extend their capabilities easily. Critically, the accommodation of new information should occur without catastrophic\n",
    "forgetting or interference.\n",
    "\n",
    "In a connectionist model, or that in which novel information is \"connected\" to an existing model, catastrophic forgetting occurs, because the data differs significantly from the data on which the model was orignially trained. Recall that as a high level definition, a machine learning alrogithm, fits mathematical equations to a dataset. While training the model on the novel dataset, the model will fit the mathematical equations on the novel data and that will likely render those equations inadequate to fit the original dataset. \n",
    "\n",
    "As an example, consider a child who spends the majority of their early years training to be a gymnast. As you know, a gymnast's body needs to be able to perform a highly specific and sophisticated set of movements. Then, as a young adult, the same person decides to become a body builder, they add several kilograms of muscle and change their body composition significantly. Ideally, the same person should be able to accomplish both tasks, but it is difficult to imagine a 300lb human performing summer saults with the same dexterity as a conventional gymnast. Now, is it likely that a human can be both a gymnast and a body builder? Perhaps, humans never cease to push the boundaries of what is possible, but it is undoubtedly a challenging endeavor. Training a Machine Learning model on novel information without interference or catastrophic forgetting is an equally challenging undertaking.   \n",
    "\n",
    "One approach to remedy catastrophic learning would be to retrain the algorithm to accomplish all tasks in one training session. However, this is not possible when the data cannot be shuffled or is observed as a continuous stream. Remember that a critical part of training a Machine Learning model is to shuffle the data so the model does not \"learn too much\" from only one part of the data. In the gymanst example, re-training the algorithm would be the equivalent of telling the gymnast to go back and re-learn how to be a gymnast while also training to be a body builder. Again, it might be possible but, costly and definitely not easily accomplished.\n",
    "\n",
    "\n",
    "### Elementary Approaches to Lifelong Machine Learning \n",
    "\n",
    "According to research coducted by Parisi et al, early approaches to lifelong machine learning typically involved the introduction of new computing resources as additional data was introduced into the problem. However, adding computing resources and interleaving new data into previoulsy trained models adds complexity and scalability issues into the overall problem. Furthermore, the purpose of lifelong learning is to allow models to adapt to new data, and to accomplish this, computational resources need to be prepared in order to protect old data, as well as provide the computational fire power to retrain the models. The problem with this approach is that there are unknown unknowns. That is, if we knew what we will need to train in the future, we would just do it all from the beginning. In short, it is impossible to determine what computational resources will be needed in the future and so simply adding more computational resources to protect already trained models becomes impractical.  \n",
    "\n",
    "\n",
    "### Regularization Approaches\n",
    "\n",
    "Generally speaking, regularization refers to setting constraints on the weights within the cost function during the training step. That is, we set constraints on the values that the weights can take on. Something akin to setting a restriction on the domain of the algorithm. Intuitively, you can say that during the training step of an additional task, we can constrain the weights in the cost function so that they are not updated too much. From a computational standpoint, catastrophic forgetting occurs when the weights of the cost function are overwritten in order to \"train\" the algorithm for the new task. However, if we are able to constrain what or by how much the weights are updated, perhaps we can prevent catasrophic forgetting. \n",
    "\n",
    "Some regularization approaches have shown promise but they can be computationally expensive and often require new tasks to be somewhat similar to the original tasks so that the training step can still converge with heavy restrictions on the weights themselves.  \n",
    "\n",
    "Broadly speaking, regularization approaches take the form of regularizing the weights so that they closely resemble the weights of the original task, penalizing large changes to the updated weights, and allowing the synapses to determine their own importance on the performance of the original task.\n",
    "\n",
    "\n",
    "#### Learning Without Forgetting (LWF) (Li & Hoiem (2016))\n",
    "\n",
    "This approach essentially transfers the knowledge from a previously trained algorithm into the training step of the new task. Concretely, the regularized weights of the original task are imported into the training step of the new task, this is meant to conserve the ability to perfrom the original task while leveraging the regularization to prevent gross deviations during the new training step. This approach is computationally expensive, however. Information needs to be conserved and recomputed during the new training step. \n",
    "\n",
    "#### Elastic Weight Consolidation (EWC) (Kirkpatrick et al. (2017))\n",
    "\n",
    "The Elastic Weight Consolodation approach essentially holds the reins during the training step in the sense that it keeps track of how much the updated weights are deviating from the original task and slows down the learning rate if the updates start getting \"out of control.\" That is, if left unchecked, a training step that makes large changes to its weights, is basically forgetting really fast. EWC aims to mitigate the speed of the catastrophic learning.  \n",
    "\n",
    "#### Synaptic Importance (Zenke, Poole, & Ganguli (2017))\n",
    "\n",
    "Similar to EWC, this approach penalizes large changes to critical components of the original task. Except, rather than penalizing the weights of the model, this approach estimates the importance of each synapsis in the performance of the model, and set heavy penalties on updates to those critical synapses.\n",
    "\n",
    "\n",
    "In summary, regularization approaches provide a way to alleviate catastrophic forgetting under certain\n",
    "conditions. However, they comprise additional loss terms for protecting consolidated knowledge\n",
    "which, with a limited amount of neural resources, may lead to a trade-off on the performance of old\n",
    "and novel tasks.\n",
    "\n",
    "#### AR1 (Maltoni & Lomonaco (2018))\n",
    "\n",
    "The AR1 model for single-incremental-task scenarios combines architectural and regularization strategies. Regularization approaches tend to progressively reduce the magnitude of weight changes batch by batch, with most of the changes occurring\n",
    "in the top layers. Instead, in AR1 intermediate layers weights are adapted without negative impact in terms of forgetting. Reported results on CORe50 (Lomonaco & Maltoni 2017) and iCIFAR-100 (Krizhevsky 2009) show that AR1 allows the training of deep convolutional models with less forgetting, outperforming LwF, EWC, and SI. (Parisi et al 2019)\n",
    "\n",
    "\n",
    "#### Transfer Learning\n",
    "\n",
    "Essentially, a transfer learning approach is an application of the LWF regularization approach. It consists of transfering the knowledge \"weights\" of previous tasks, into the learning step of the new task, and using regularizations to control the extent by which the weights are updated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
