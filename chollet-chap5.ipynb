{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "################################################################################\n",
    "# Copyright (c) 2019. Vincenzo Lomonaco, Massimo Caccia, Pau Rodriguez,        #\n",
    "# Lorenzo Pellegrini. All rights reserved.                                     #\n",
    "# Copyrights licensed under the CC BY 4.0 License.                             #\n",
    "# See the accompanying LICENSE file for terms.                                 #\n",
    "#                                                                              #\n",
    "# Date: 1-02-2019                                                              #\n",
    "# Author: Vincenzo Lomonaco                                                    #\n",
    "# E-mail: vincenzo.lomonaco@unibo.it                                           #\n",
    "# Website: vincenzolomonaco.com                                                #\n",
    "################################################################################\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Basic data loader for the CVPR2020 CLVision Challenge.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Python 2-3 compatible\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import pickle as pkl\n",
    "import logging\n",
    "from hashlib import md5\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/core50_imgs.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-526eb9722420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the dataset object for example with the \"multi-task-nc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# and assuming the core50 location in ~/core50/128x128/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCORE50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multi-task-nc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# Get the fixed valid set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-526eb9722420>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, preload, scenario, cumul, run, start_batch, task_sep)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'core50_imgs.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                     \u001b[0mnpzfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpzfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/core50_imgs.npz'"
     ]
    }
   ],
   "source": [
    "class CORE50(object):\n",
    "    \"\"\" CORe50 CLVision Challenge Data Loader.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of the dataset where ``core50_128x128``,\n",
    "            ``paths.pkl``, ``LUP.pkl``, ``labels.pkl``, ``core50_imgs.npz``\n",
    "            live. For example ``~/data/core50``.\n",
    "        preload (string, optional): If True data is pre-loaded with look-up\n",
    "            tables. RAM usage may be high.\n",
    "        scenario (string, optional): One of the three scenarios of the CORe50\n",
    "            benchmark ``ni``, ``multi-task-nc``, ``nic``.\n",
    "        train (bool, optional): If True, creates the dataset from the training\n",
    "            set, otherwise creates from test set.\n",
    "        cumul (bool, optional): If True the cumulative scenario is assumed, the\n",
    "            incremental scenario otherwise. Practically speaking ``cumul=True``\n",
    "            means that for batch=i also batch=0,...i-1 will be added to the\n",
    "            available training data.\n",
    "        run (int, optional): One of the 10 runs (from 0 to 9) in which the\n",
    "            training batch order is changed as in the official benchmark.\n",
    "        start_batch (int, optional): One of the training incremental batches\n",
    "            from 0 to max-batch - 1. Remember that for the ``ni``,\n",
    "            ``multi-task-nc`` and ``nic`` we have respectively 8, 9 and 391\n",
    "            incremental batches. If ``train=False`` this parameter will be\n",
    "            ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    new2old_names = {'ni': 'ni', 'multi-task-nc': 'nc', 'nic': 'nicv2_391'}\n",
    "    nbatch = {\n",
    "        'ni': 8,\n",
    "        'nc': 9,\n",
    "        'nicv2_391': 391\n",
    "    }\n",
    "\n",
    "    def __init__(self, root='', preload=False, scenario='ni', cumul=False,\n",
    "                 run=0, start_batch=0, task_sep=False):\n",
    "        \"\"\"\" Initialize Object \"\"\"\n",
    "\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.preload = preload\n",
    "        self.scenario = self.new2old_names[scenario]\n",
    "        self.cumul = cumul\n",
    "        self.run = run\n",
    "        self.batch = start_batch\n",
    "        self.task_sep = task_sep\n",
    "\n",
    "        if preload:\n",
    "            print(\"Loading data...\")\n",
    "            bin_path = os.path.join(root, 'core50_imgs.bin')\n",
    "            if os.path.exists(bin_path):\n",
    "                with open(bin_path, 'rb') as f:\n",
    "                    self.x = np.fromfile(f, dtype=np.uint8) \\\n",
    "                        .reshape(164866, 128, 128, 3)\n",
    "\n",
    "            else:\n",
    "                with open(os.path.join(root, 'core50_imgs.npz'), 'rb') as f:\n",
    "                    npzfile = np.load(f)\n",
    "                    self.x = npzfile['x']\n",
    "                    print(\"Writing bin for fast reloading...\")\n",
    "                    self.x.tofile(bin_path)\n",
    "\n",
    "        print(\"Loading paths...\")\n",
    "        with open(os.path.join(root, 'paths.pkl'), 'rb') as f:\n",
    "            self.paths = pkl.load(f)\n",
    "\n",
    "        print(\"Loading LUP...\")\n",
    "        with open(os.path.join(root, 'LUP.pkl'), 'rb') as f:\n",
    "            self.LUP = pkl.load(f)\n",
    "\n",
    "        print(\"Loading labels...\")\n",
    "        with open(os.path.join(root, 'labels.pkl'), 'rb') as f:\n",
    "            self.labels = pkl.load(f)\n",
    "\n",
    "        # to be changed\n",
    "        self.tasks_id = []\n",
    "        self.labs_for_task = []\n",
    "\n",
    "        if self.scenario == 'nc':\n",
    "            self.task_sep = True\n",
    "        else:\n",
    "            self.task_sep = False\n",
    "\n",
    "        print(\"preparing CL benchmark...\")\n",
    "        for i in range(self.nbatch[self.scenario]):\n",
    "            if self.task_sep:\n",
    "                self.tasks_id.append(i)\n",
    "                self.labs_for_task.append(\n",
    "                    list(set(self.labels[self.scenario][run][i]))\n",
    "                )\n",
    "            else:\n",
    "                self.tasks_id.append(0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\" Next batch based on the object parameter which can be also changed\n",
    "            from the previous iteration. \"\"\"\n",
    "\n",
    "        scen = self.scenario\n",
    "        run = self.run\n",
    "        batch = self.batch\n",
    "\n",
    "        if self.batch == self.nbatch[scen]:\n",
    "            raise StopIteration\n",
    "\n",
    "        # Getting the right indexis\n",
    "        if self.cumul:\n",
    "            train_idx_list = []\n",
    "            for i in range(self.batch + 1):\n",
    "                train_idx_list += self.LUP[scen][run][i]\n",
    "        else:\n",
    "            train_idx_list = self.LUP[scen][run][batch]\n",
    "\n",
    "        # loading data\n",
    "        if self.preload:\n",
    "            train_x = np.take(self.x, train_idx_list, axis=0)\\\n",
    "                      .astype(np.float32)\n",
    "        else:\n",
    "            print(\"Loading data...\")\n",
    "            # Getting the actual paths\n",
    "            train_paths = []\n",
    "            for idx in train_idx_list:\n",
    "                train_paths.append(os.path.join(self.root, self.paths[idx]))\n",
    "            # loading imgs\n",
    "            train_x = self.get_batch_from_paths(train_paths).astype(np.float32)\n",
    "\n",
    "        # In either case we have already loaded the y\n",
    "        if self.cumul:\n",
    "            train_y = []\n",
    "            for i in range(self.batch + 1):\n",
    "                train_y += self.labels[scen][run][i]\n",
    "        else:\n",
    "            train_y = self.labels[scen][run][batch]\n",
    "\n",
    "        train_y = np.asarray(train_y, dtype=np.float32)\n",
    "\n",
    "        # Update state for next iter\n",
    "        self.batch += 1\n",
    "\n",
    "        return train_x, train_y, self.tasks_id[self.batch-1]\n",
    "\n",
    "    next = __next__  # python2.x compatibility.\n",
    "\n",
    "    def get_full_valid_set(self, reduced=True):\n",
    "        \"\"\"\n",
    "        Return the test set (the same for each inc. batch).\n",
    "        \"\"\"\n",
    "\n",
    "        scen = self.scenario\n",
    "        run = self.run\n",
    "        valid_idx_list = self.LUP[scen][run][-1]\n",
    "        valid_y = self.labels[scen][run][-1]\n",
    "\n",
    "        if self.scenario == 'nc':\n",
    "\n",
    "            valid_set = []\n",
    "            i_valid_paths = {i:[] for i in range(self.nbatch[self.scenario])}\n",
    "            idx_x_task = {i:[] for i in range(self.nbatch[self.scenario])}\n",
    "            y_x_task = {i:[] for i in range(self.nbatch[self.scenario])}\n",
    "\n",
    "            for idx, y in zip(valid_idx_list, valid_y):\n",
    "                for i in range(self.nbatch[self.scenario]):\n",
    "                    if y in self.labs_for_task[i]:\n",
    "                        idx_x_task[i].append(idx)\n",
    "                        y_x_task[i].append(y)\n",
    "                        i_valid_paths[i].append(\n",
    "                            os.path.join(self.root, self.paths[idx]))\n",
    "\n",
    "            for i in range(self.nbatch[self.scenario]):\n",
    "                if self.preload:\n",
    "                    i_valid_x = np.take(self.x, idx_x_task[i], axis=0)\\\n",
    "                        .astype(np.float32)\n",
    "                    # print(i, len(idx_x_task[i]))\n",
    "                else:\n",
    "                    i_valid_x = self.get_batch_from_paths(i_valid_paths[i])\\\n",
    "                        .astype(np.float32)\n",
    "                i_valid_y = np.asarray(y_x_task[i], dtype=np.float32)\n",
    "\n",
    "                if reduced:\n",
    "                    # reduce valid set by 20\n",
    "                    idx = range(0, i_valid_y.shape[0], 20)\n",
    "                    i_valid_x = np.take(i_valid_x, idx, axis=0)\n",
    "                    i_valid_y = np.take(i_valid_y, idx, axis=0)\n",
    "\n",
    "                valid_set.append([(i_valid_x, i_valid_y), i])\n",
    "\n",
    "\n",
    "        else:\n",
    "            if self.preload:\n",
    "                valid_x = np.take(self.x, valid_idx_list, axis=0)\\\n",
    "                    .astype(np.float32)\n",
    "            else:\n",
    "                # test paths\n",
    "                valid_paths = []\n",
    "                for idx in valid_idx_list:\n",
    "                    valid_paths.append(os.path.join(self.root, self.paths[idx]))\n",
    "\n",
    "                # test imgs\n",
    "                valid_x = self.get_batch_from_paths(valid_paths)\\\n",
    "                    .astype(np.float32)\n",
    "\n",
    "            valid_y = np.asarray(valid_y, dtype=np.float32)\n",
    "\n",
    "            if reduced:\n",
    "                # reduce valid set by 20\n",
    "                idx = range(0, valid_y.shape[0], 20)\n",
    "                valid_x = np.take(valid_x, idx, axis=0)\n",
    "                valid_y = np.take(valid_y, idx, axis=0)\n",
    "\n",
    "            valid_set = [[(valid_x, valid_y), self.tasks_id[self.batch - 1]]]\n",
    "\n",
    "        return valid_set\n",
    "\n",
    "    def get_full_test_set(self):\n",
    "        \"\"\"\n",
    "        Return the full test set (no labels)\n",
    "        \"\"\"\n",
    "\n",
    "        filelist_path = self.root + \"test_filelist.txt\"\n",
    "        filelist_tlabeled_path = self.root + \"test_filelist_tlabeled.txt\"\n",
    "        test_img_dir = self.root + \"core50_challenge_test\"\n",
    "\n",
    "        if self.scenario == 'nc':\n",
    "            test_paths = {i:[] for i in range(self.nbatch[self.scenario])}\n",
    "            with open(filelist_tlabeled_path, \"r\") as rf:\n",
    "                for line in rf:\n",
    "                    path, task_label = line.split()\n",
    "                    test_paths[int(task_label.strip())].append(\n",
    "                        os.path.join(test_img_dir, path.strip()))\n",
    "\n",
    "            full_test = []\n",
    "            for i in range(self.nbatch[self.scenario]):\n",
    "                test_x = self.get_batch_from_paths(test_paths[i])\\\n",
    "                    .astype(np.float32)\n",
    "                full_test.append(\n",
    "                    [(test_x, np.asarray([-1] * len(test_paths[i]))), i])\n",
    "\n",
    "        else:\n",
    "            test_paths = []\n",
    "            with open(filelist_path, \"r\") as rf:\n",
    "                for line in rf:\n",
    "                    test_paths.append(os.path.join(test_img_dir, line.strip()))\n",
    "            test_x = self.get_batch_from_paths(test_paths).astype(np.float32)\n",
    "\n",
    "            full_test = [[(test_x, np.asarray([-1] * len(test_paths))), 0]]\n",
    "\n",
    "        return full_test\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_batch_from_paths(paths, compress=False, snap_dir='',\n",
    "                             on_the_fly=True, verbose=False):\n",
    "        \"\"\" Given a number of abs. paths it returns the numpy array\n",
    "        of all the images. \"\"\"\n",
    "\n",
    "        # Getting root logger\n",
    "        log = logging.getLogger('mylogger')\n",
    "\n",
    "        # If we do not process data on the fly we check if the same train\n",
    "        # filelist has been already processed and saved. If so, we load it\n",
    "        # directly. In either case we end up returning x and y, as the full\n",
    "        # training set and respective labels.\n",
    "        num_imgs = len(paths)\n",
    "        hexdigest = md5(''.join(paths).encode('utf-8')).hexdigest()\n",
    "        log.debug(\"Paths Hex: \" + str(hexdigest))\n",
    "        loaded = False\n",
    "        x = None\n",
    "        file_path = None\n",
    "\n",
    "        if compress:\n",
    "            file_path = snap_dir + hexdigest + \".npz\"\n",
    "            if os.path.exists(file_path) and not on_the_fly:\n",
    "                loaded = True\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    npzfile = np.load(f)\n",
    "                    x, y = npzfile['x']\n",
    "        else:\n",
    "            x_file_path = snap_dir + hexdigest + \"_x.bin\"\n",
    "            if os.path.exists(x_file_path) and not on_the_fly:\n",
    "                loaded = True\n",
    "                with open(x_file_path, 'rb') as f:\n",
    "                    x = np.fromfile(f, dtype=np.uint8) \\\n",
    "                        .reshape(num_imgs, 128, 128, 3)\n",
    "\n",
    "        # Here we actually load the images.\n",
    "        if not loaded:\n",
    "            # Pre-allocate numpy arrays\n",
    "            x = np.zeros((num_imgs, 128, 128, 3), dtype=np.uint8)\n",
    "\n",
    "            for i, path in enumerate(paths):\n",
    "                if verbose:\n",
    "                    print(\"\\r\" + path + \" processed: \" + str(i + 1), end='')\n",
    "\n",
    "                x[i] = np.array(Image.open(path))\n",
    "\n",
    "            if verbose:\n",
    "                print()\n",
    "\n",
    "            if not on_the_fly:\n",
    "                # Then we save x\n",
    "                if compress:\n",
    "                    with open(file_path, 'wb') as g:\n",
    "                        np.savez_compressed(g, x=x)\n",
    "                else:\n",
    "                    x.tofile(snap_dir + hexdigest + \"_x.bin\")\n",
    "\n",
    "        assert (x is not None), 'Problems loading data. x is None!'\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create the dataset object for example with the \"multi-task-nc\"\n",
    "    # and assuming the core50 location in ~/core50/128x128/\n",
    "    dataset = CORE50(root='~/cvpr_clvision_challenge/core50/data/', scenario=\"multi-task-nc\", preload=True)\n",
    "\n",
    "    # Get the fixed valid set\n",
    "    print(\"Recovering validation set...\")\n",
    "    full_valdiset = dataset.get_full_valid_set()\n",
    "    # Get the fixed test set\n",
    "    print(\"Recovering test set...\")\n",
    "    full_testset = dataset.get_full_test_set()\n",
    "\n",
    "    # loop over the training incremental batches\n",
    "    for i, (x, y, t) in enumerate(dataset):\n",
    "\n",
    "        print(\"----------- batch {0} -------------\".format(i))\n",
    "        print(\"x shape: {0}, y: {0}\"\n",
    "              .format(x.shape, y.shape))\n",
    "\n",
    "        # use the data\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
